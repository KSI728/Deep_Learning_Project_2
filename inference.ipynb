{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "827bad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\knudc\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa8dfb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "model = GPT2LMHeadModel.from_pretrained('output/checkpoint-260').to(device)\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('skt/kogpt2-base-v2',\n",
    "bos_token='<s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd2c1e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"날으는 새처럼\"\n",
    "input = tokenizer.encode(word, return_tensors = \"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        input_ids=input,\n",
    "        temperature=0.9,\n",
    "        min_length=64,\n",
    "        max_length=256,\n",
    "        repetition_penalty=1.2,\n",
    "        do_sample=True,\n",
    "        early_stopping=True,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        top_k=10,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    \n",
    "generated_sentence = output[0].tolist()\n",
    "text = tokenizer.decode(generated_sentence, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "080f19d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "날으는 새처럼\n",
      "나는 갈매기에게 가랑잎을 맡기고\n",
      "가랑잎은 밟아본다.\n",
      "바람도 불어오지 않은 어느 날,\n",
      "내가 사는 곳은\n",
      "내 집이었다.\n",
      "그러나 나는, 그 자리에 누울 수가 없었다.\n",
      "그날 밤, 바람이 불고 가는 밤이면 \n",
      "이제야 나를 알았으니까.\n"
     ]
    }
   ],
   "source": [
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
